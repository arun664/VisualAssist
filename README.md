# ğŸ¤– AI Navigation Assistant

Real-time AI-powered navigation assistance with object detection, path guidance, and audio feedback using **pretrained YOLO models**.

> **ğŸš€ New to this project?** Check [QUICK_START.md](QUICK_START.md) for 2-step setup!

## ğŸš€ Production Setup (Recommended)

### 1. Backend Deployed on AWS
Backend is running on AWS: `http://18.222.141.234:8000`

### 2. Use Live Frontend & Client
- **Frontend**: https://arun664.github.io/VisualAssist/
- **Client**: https://arun664.github.io/VisualAssist/client/

**âœ¨ Zero setup needed!** Both frontend/client (GitHub Pages) and backend (AWS) are fully hosted in the cloud.

## ğŸ› ï¸ Local Development Setup

### 1. Install Dependencies
```bash
cd backend
pip install -r requirements.txt
```

### 2. Start Backend Locally
```bash
cd backend
python main.py
```

### 3. Start Frontend & Client Locally
```bash
# Frontend (Terminal 1)
cd frontend && python -m http.server 3000

# Client (Terminal 2) 
cd client && python -m http.server 3001
```

- **Frontend**: http://localhost:3000
- **Client**: http://localhost:3001

## ğŸ§  AI Models (Pretrained)

The system uses **pretrained YOLOv11 models** from ultralytics:

| Model | Size | Speed | Accuracy | Use Case |
|-------|------|-------|----------|----------|
| `yolo11n.pt` | 2.6MB | âš¡ Fastest | Good | **Default** - Real-time navigation |
| `yolo11s.pt` | 9.4MB | Fast | Better | Balanced performance |
| `yolo11m.pt` | 20MB | Medium | High | More accurate detection |
| `yolo11l.pt` | 25MB | Slower | Higher | High accuracy needs |
| `yolo11x.pt` | 56MB | Slowest | Highest | Best possible accuracy |

**Default**: `yolo11n.pt` (nano) for optimal real-time performance.

## ğŸ¯ Object Detection Capabilities

The AI detects **40+ navigation-relevant objects**:

### ğŸª‘ Furniture & Obstacles
- Chairs, tables, couches, beds
- Potted plants, TVs, laptops
- Bags, bottles, sports equipment

### ğŸ‘¥ People & Animals  
- Persons, dogs, cats, horses
- Moving and stationary detection

### ğŸš— Vehicles
- Cars, motorcycles, trucks, buses
- Bicycles, skateboards

### ğŸ  Household Items
- Refrigerators, microwaves, sinks
- Books, clocks, vases, phones

## ğŸ”§ Configuration

### Change AI Model
Edit `backend/computer_vision.py`:
```python
# For faster processing (default)
vision_processor = VisionProcessor(model_name="yolo11n.pt")

# For better accuracy
vision_processor = VisionProcessor(model_name="yolo11m.pt")
```

### Adjust Detection Sensitivity
```python
vision_processor = VisionProcessor(
    model_name="yolo11n.pt",
    confidence_threshold=0.5,  # Lower = more detections
    stationary_threshold=2.0   # Motion sensitivity
)
```

## ğŸŒ Production Architecture

```
GitHub Pages (HTTPS)          AWS Instance (HTTP)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Frontend       â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”¤  Backend         â”‚
â”‚  Client         â”‚  CORS   â”‚  + Pretrained AI â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Benefits:
- âœ… **Free frontend hosting** (GitHub Pages)
- âœ… **Cloud backend hosting** (AWS)
- âœ… **Scalable AI processing** (AWS infrastructure)
- âœ… **No model files to manage** (auto-download)
- âœ… **Simple deployment** (GitHub Pages + AWS)
- âœ… **Zero local setup** (fully cloud-hosted)

## ğŸ§ª Test Backend

```bash
# Check AWS backend health
curl http://18.222.141.234:8000/health
```

## ğŸ” How It Works

### 1. **Pretrained Model Loading**
```python
from ultralytics import YOLO

# Automatically downloads if not cached
model = YOLO("yolo11n.pt")
```

### 2. **Object Detection**
- Detects 80 COCO classes
- Filters for navigation-relevant objects
- Ignores ceiling/roof objects (top 30% of frame)
- Focuses on ground-level obstacles

### 3. **Path Calculation**
- Creates 6x8 grid for path analysis
- Marks safe vs blocked areas
- Calculates walking corridors
- Provides visual + audio guidance

### 4. **Real-time Processing**
- 0.5 FPS for optimal performance
- Circuit breaker for error handling
- Automatic model caching

## ğŸš¨ Troubleshooting

### Model Download Issues
```bash
# Test internet connection
ping github.com

# Clear ultralytics cache
rm -rf ~/.cache/ultralytics

# Reinstall ultralytics
pip uninstall ultralytics
pip install ultralytics
```

### CORS Issues
**Production (AWS)**: CORS is configured for HTTP connections from GitHub Pages.

**If CORS issues occur**: 
1. **Use Chrome with CORS disabled**:
   ```
   chrome.exe --disable-web-security --user-data-dir="C:\temp\chrome-cors"
   ```

2. **Or restart local backend**:
   ```bash
   docker-compose restart backend
   ```

### Performance Issues
- **Switch to faster model**: Use `yolo11n.pt` (default)
- **Reduce frame rate**: Already optimized to 0.5 FPS
- **Check CPU usage**: AI processing is CPU-intensive

## ğŸ“± Usage Instructions

### Client Setup:
1. Open: https://arun664.github.io/VisualAssist/client/
2. Click "Enable Camera" â†’ Allow permissions
3. Click "Enable Microphone" â†’ Allow permissions  
4. Click "Start Streaming" â†’ Connects to AWS backend

### Frontend Monitor:
1. Open: https://arun664.github.io/VisualAssist/
2. See real-time AI processed video
3. Click "Start Navigation" â†’ Audio guidance activates
4. Click "Stop Navigation" â†’ Audio mutes, video continues

### What You'll Hear:
- **"Clear path ahead"** â†’ Safe to move forward
- **"Chair detected"** â†’ Obstacle identified
- **"Path blocked"** â†’ Multiple obstacles
- **"Obstacle on left/right"** â†’ Directional guidance

## ğŸ”’ Privacy & Security

- âœ… **Secure cloud processing** - HTTPS encrypted connections
- âœ… **Pretrained models** - No training data exposure  
- âœ… **HTTPS everywhere** - End-to-end encryption
- âœ… **No data storage** - Real-time processing only
- âœ… **Railway security** - Enterprise-grade infrastructure

## ğŸš€ Deployment Setup

### GitHub Pages Setup (One-time)
1. Go to repository **Settings** â†’ **Pages**
2. Set **Source** to **GitHub Actions**
3. Go to **Actions** â†’ **General** â†’ **Workflow permissions**
4. Select **Read and write permissions**
5. Push to `main` branch to trigger deployment

### Live URLs
- **Frontend**: `https://arun664.github.io/VisualAssist/`
- **Client**: `https://arun664.github.io/VisualAssist/client/`
- **Backend**: `http://18.222.141.234:8000` (AWS)

### Local Development
```bash
# Start local backend
docker-compose up -d backend

# Check status
docker-compose ps
docker-compose logs backend
```

## ğŸ“ Project Structure

```
ai-navigation-assistant/
â”œâ”€â”€ backend/                 # Docker backend service
â”‚   â”œâ”€â”€ main.py             # FastAPI application
â”‚   â”œâ”€â”€ requirements.txt    # Python dependencies
â”‚   â””â”€â”€ Dockerfile         # Docker configuration
â”œâ”€â”€ frontend/               # GitHub Pages frontend
â”‚   â”œâ”€â”€ index.html         # Main interface
â”‚   â”œâ”€â”€ app.js            # Frontend logic
â”‚   â””â”€â”€ styles.css        # Styling
â”œâ”€â”€ client/                # GitHub Pages client
â”‚   â”œâ”€â”€ index.html        # Client interface
â”‚   â”œâ”€â”€ client.js         # WebRTC streaming
â”‚   â””â”€â”€ client-styles.css # Client styling
â”œâ”€â”€ .github/workflows/     # GitHub Actions
â”œâ”€â”€ docker-compose.yml     # Local development
â”œâ”€â”€ config.js             # Configuration
â””â”€â”€ README.md            # This file
```

## ğŸ“„ License

MIT License - Free for personal and commercial use.

---

**Powered by:** Ultralytics YOLOv11, FastAPI, WebRTC, AWS, and GitHub Pages.